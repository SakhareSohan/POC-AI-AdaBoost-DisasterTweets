{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leRRqzbxHiZm",
        "outputId": "b53a015f-834b-478b-c707-36ab9eff8ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Define a function for data processing (preprocessing, removing stop words, and lemmatization)\n",
        "def data_processing(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"https\\S+|www\\S+https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@w+|\\#', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text_tokens = word_tokenize(text)\n",
        "\n",
        "    # Define a lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Apply lemmatization and remove stop words\n",
        "    processed_text = [lemmatizer.lemmatize(word) for word in text_tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    return \" \".join(processed_text)\n",
        "\n",
        "# Apply data processing to your dataset\n",
        "df['processed_text'] = df['text'].apply(data_processing)\n",
        "\n",
        "X = df['processed_text']  # We will use the stemmed text for further processing\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define a function to get sentiment scores\n",
        "def get_sentiment(text):\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']  # We'll use the compound score as the sentiment feature\n",
        "\n",
        "# Function to train and evaluate the LSTM model with predefined disaster keywords\n",
        "def train_and_evaluate_lstm(predefined_disasters=None):\n",
        "    # Apply data processing and sentiment analysis to your dataset\n",
        "    df['processed_text'] = df['text'].apply(data_processing)\n",
        "    df['sentiment'] = df['text'].apply(get_sentiment)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X = df['processed_text']\n",
        "    y = df['target']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define and compile the LSTM model\n",
        "    lstm_model = keras.Sequential([\n",
        "        Embedding(input_dim=10000, output_dim=128, input_length=50),\n",
        "        LSTM(128),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Tokenize and pad the text data\n",
        "    tokenizer = Tokenizer(num_words=10000)\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=50)\n",
        "    X_test_padded = pad_sequences(X_test_seq, maxlen=50)\n",
        "\n",
        "    # Train the LSTM model\n",
        "    lstm_model.fit(X_train_padded, y_train, epochs=5, batch_size=64, verbose=1)\n",
        "\n",
        "    # Save the tokenizer with the word index\n",
        "    tokenizer_path = 'tokenizer.pkl'\n",
        "    joblib.dump(tokenizer, tokenizer_path)\n",
        "\n",
        "    # Evaluate the LSTM model\n",
        "    accuracy = lstm_model.evaluate(X_test_padded, y_test, verbose=0)\n",
        "    print(\"LSTM Model Accuracy:\", accuracy[1])  # Accuracy is the second element of the evaluation result\n",
        "train_and_evaluate_lstm()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3v4hWtyfGdb",
        "outputId": "e4786d6e-2db7-4e0b-88e3-f4c0f40e9c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "96/96 [==============================] - 26s 211ms/step - loss: 0.5565 - accuracy: 0.7107\n",
            "Epoch 2/5\n",
            "96/96 [==============================] - 15s 152ms/step - loss: 0.3178 - accuracy: 0.8693\n",
            "Epoch 3/5\n",
            "96/96 [==============================] - 17s 179ms/step - loss: 0.2018 - accuracy: 0.9212\n",
            "Epoch 4/5\n",
            "96/96 [==============================] - 15s 158ms/step - loss: 0.1366 - accuracy: 0.9491\n",
            "Epoch 5/5\n",
            "96/96 [==============================] - 15s 154ms/step - loss: 0.1048 - accuracy: 0.9657\n",
            "LSTM Model Accuracy: 0.7695338129997253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained LSTM model to a file\n",
        "lstm_model.save('lstm_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vskGbKcnBbf",
        "outputId": "f70c5dfd-c276-4661-9e0f-38df4ecbac62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define a function to get sentiment scores\n",
        "def get_sentiment(text):\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']  # We'll use the compound score as the sentiment feature\n",
        "\n",
        "# Function to train and evaluate a Naive Bayes model with predefined disaster keywords\n",
        "def train_and_evaluate_naive_bayes(df, predefined_disasters=None):\n",
        "    # Apply data processing and sentiment analysis to your dataset\n",
        "    df['processed_text'] = df['text'].apply(data_processing)\n",
        "    df['sentiment'] = df['text'].apply(get_sentiment)\n",
        "\n",
        "    # Combine predefined disaster keywords with the processed text\n",
        "    if predefined_disasters:\n",
        "        df['text'] = df['text'] + ' ' + df['processed_text'].apply(lambda text: ' '.join([word for word in text.split() if word in predefined_disasters]))\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X = df['text']\n",
        "    y = df['target']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create a TF-IDF vectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2))\n",
        "\n",
        "    # Transform the text data into TF-IDF features\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    # Initialize and train a Naive Bayes model\n",
        "    nb_model = MultinomialNB()\n",
        "    nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    nb_pred = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "    # Evaluate the Naive Bayes model\n",
        "    accuracy_nb = accuracy_score(y_test, nb_pred)\n",
        "    classification_rep_nb = classification_report(y_test, nb_pred)\n",
        "\n",
        "    print(\"Naive Bayes Model Accuracy:\", accuracy_nb)\n",
        "    print(\"Naive Bayes Model Classification Report:\\n\", classification_rep_nb)\n",
        "\n",
        "# Call the function to train and evaluate the Naive Bayes model\n",
        "train_and_evaluate_naive_bayes(df, predefined_disasters=[\"earthquake\", \"flood\", \"fire\", \"hurricane\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdt2Dvfdnz7A",
        "outputId": "85d8e783-f0dd-4e6b-e6d4-d0d88f2f97bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Model Accuracy: 0.7964543663821405\n",
            "Naive Bayes Model Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84       874\n",
            "           1       0.84      0.65      0.73       649\n",
            "\n",
            "    accuracy                           0.80      1523\n",
            "   macro avg       0.81      0.78      0.78      1523\n",
            "weighted avg       0.80      0.80      0.79      1523\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define a function to get sentiment scores\n",
        "def get_sentiment(text):\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']  # We'll use the compound score as the sentiment feature\n",
        "\n",
        "# Function to train and evaluate an SVM model with predefined disaster keywords\n",
        "def train_and_evaluate_svm(df, predefined_disasters=None):\n",
        "    # Apply data processing and sentiment analysis to your dataset\n",
        "    df['processed_text'] = df['text'].apply(data_processing)\n",
        "    df['sentiment'] = df['text'].apply(get_sentiment)\n",
        "\n",
        "    # Combine predefined disaster keywords with the processed text\n",
        "    if predefined_disasters:\n",
        "        df['text'] = df['text'] + ' ' + df['processed_text'].apply(lambda text: ' '.join([word for word in text.split() if word in predefined_disasters]))\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X = df['text']\n",
        "    y = df['target']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create a TF-IDF vectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2))\n",
        "\n",
        "    # Transform the text data into TF-IDF features\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    # Initialize and train an SVM model\n",
        "    svm_model = SVC(kernel='linear')\n",
        "    svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    svm_pred = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "    # Evaluate the SVM model\n",
        "    accuracy_svm = accuracy_score(y_test, svm_pred)\n",
        "    classification_rep_svm = classification_report(y_test, svm_pred)\n",
        "\n",
        "    print(\"SVM Model Accuracy:\", accuracy_svm)\n",
        "    print(\"SVM Model Classification Report:\\n\", classification_rep_svm)\n",
        "\n",
        "# Call the function to train and evaluate the SVM model\n",
        "train_and_evaluate_svm(df, predefined_disasters=[\"earthquake\", \"flood\", \"fire\", \"hurricane\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVIcGgWipO7_",
        "outputId": "ee79e08e-e2bb-416b-c140-c21c3f974e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Model Accuracy: 0.8049901510177282\n",
            "SVM Model Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       874\n",
            "           1       0.80      0.72      0.76       649\n",
            "\n",
            "    accuracy                           0.80      1523\n",
            "   macro avg       0.80      0.79      0.80      1523\n",
            "weighted avg       0.80      0.80      0.80      1523\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the Naive Bayes model\n",
        "joblib.dump(nb_model, 'naive_bayes_model.pkl')\n",
        "\n",
        "# Save the SVM model\n",
        "joblib.dump(svm_model, 'svm_model.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9r2Dy_gqGJq",
        "outputId": "61a86cb2-4eda-4948-baeb-31884c1e8b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svm_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import joblib\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the saved tokenizer\n",
        "tokenizer_path = 'tokenizer.pkl'\n",
        "with open(tokenizer_path, 'rb') as tokenizer_file:\n",
        "    tokenizer = joblib.load(tokenizer_file)\n",
        "\n",
        "# Load the saved model\n",
        "lstm_model = load_model('lstm_model.h5')\n",
        "naive_bayes_model = joblib.load('naive_bayes_model.pkl')\n",
        "svm_model = joblib.load('svm_model.pkl')\n",
        "\n",
        "# Preprocess input comment\n",
        "def data_processing(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"https\\S+|www\\S+https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@w+|\\#', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text_tokens = word_tokenize(text)\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    processed_text = [lemmatizer.lemmatize(word) for word in text_tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    return \" \".join(processed_text)\n",
        "\n",
        "input_comment = \"aeroplane turbulence\"\n",
        "preprocessed_input = data_processing(input_comment)\n",
        "\n",
        "# Use each model to make individual predictions\n",
        "input_seq = tokenizer.texts_to_sequences([preprocessed_input])\n",
        "input_padded = pad_sequences(input_seq, maxlen=50)\n",
        "\n",
        "lstm_prediction = lstm_model.predict(input_padded)\n",
        "naive_bayes_prediction = naive_bayes_model.predict([preprocessed_input])[0]\n",
        "svm_prediction = svm_model.predict([preprocessed_input])[0]\n",
        "\n",
        "# Display individual predictions\n",
        "print(\"LSTM Model Prediction:\", lstm_prediction)\n",
        "print(\"Naive Bayes Model Prediction:\", naive_bayes_prediction)\n",
        "print(\"SVM Model Prediction:\", svm_prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TohOrf79qYly",
        "outputId": "75c5b722-f272-4e4a-9c9b-8ecc03485c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 489ms/step\n",
            "LSTM Model Prediction: [[0.17883675]]\n",
            "Naive Bayes Model Prediction: 0\n",
            "SVM Model Prediction: 0\n"
          ]
        }
      ]
    }
  ]
}